{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from http import HTTPStatus\n",
    "from typing import Annotated\n",
    "from uuid import UUID\n",
    "\n",
    "from fastapi import Depends, FastAPI, HTTPException, WebSocket\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_elasticsearch import ApproxRetrievalStrategy, ElasticsearchStore\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from core_api.src.auth import get_user_uuid\n",
    "from redbox.llm.prompts.chat import (\n",
    "    CONDENSE_QUESTION_PROMPT,\n",
    "    STUFF_DOCUMENT_PROMPT,\n",
    "    WITH_SOURCES_PROMPT,\n",
    ")\n",
    "from redbox.model_db import MODEL_PATH\n",
    "from redbox.models import EmbeddingModelInfo, Settings\n",
    "from redbox.models.chat import ChatRequest, ChatResponse, SourceDocument\n",
    "\n",
    "\n",
    "env = Settings(_env_file=\"../.env\")\n",
    "\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=env.embedding_model, cache_folder=\"../models/\")\n",
    "\n",
    "es = Elasticsearch(\n",
    "    hosts=[\n",
    "        {\n",
    "            \"host\": \"localhost\",\n",
    "            \"port\": env.elastic.port,\n",
    "            \"scheme\": env.elastic.scheme,\n",
    "        }\n",
    "    ],\n",
    "    basic_auth=(env.elastic.user, env.elastic.password),\n",
    ")\n",
    "\n",
    "if env.elastic.subscription_level == \"basic\":\n",
    "    strategy = ApproxRetrievalStrategy(hybrid=False)\n",
    "elif env.elastic.subscription_level in [\"platinum\", \"enterprise\"]:\n",
    "    strategy = ApproxRetrievalStrategy(hybrid=True)\n",
    "\n",
    "vector_store = ElasticsearchStore(\n",
    "    es_connection=es,\n",
    "    index_name=\"redbox-data-chunk\",\n",
    "    embedding=embedding_model,\n",
    "    strategy=strategy,\n",
    "    vector_query_field=\"embedding\",\n",
    ")\n",
    "\n",
    "llm = ChatLiteLLM(\n",
    "    model=env.openai_model,\n",
    "    streaming=True,\n",
    "    azure_key=env.azure_openai_api_key,\n",
    "    api_version=env.openai_api_version,\n",
    "    api_base=env.azure_openai_endpoint,\n",
    "    max_tokens=4_096,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarisation scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from redbox.models.file import Metadata\n",
    "from functools import reduce\n",
    "from redbox.storage import ElasticsearchStorageHandler\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.schema import StrOutputParser\n",
    "from redbox.llm.prompts.core import _core_redbox_prompt\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_handler = ElasticsearchStorageHandler(es_client=es, root_index=env.elastic_root_index)\n",
    "\n",
    "def get_file_as_documents(\n",
    "    file_uuid: UUID,\n",
    "    user_uuid: UUID,\n",
    "    storage_handler: ElasticsearchStorageHandler = storage_handler,\n",
    "    max_tokens: int | None = None\n",
    ") -> list[Document]:\n",
    "    \"\"\"Gets a file as LangChain Documents, splitting it by max_tokens.\"\"\"\n",
    "    documents: list[Document] = []\n",
    "    chunks_unsorted = storage_handler.get_file_chunks(parent_file_uuid=file_uuid, user_uuid=user_uuid)\n",
    "    chunks = sorted(chunks_unsorted, key=lambda x: x.index)\n",
    "\n",
    "    total_tokens = sum(chunk.token_count for chunk in chunks)\n",
    "    print(total_tokens)\n",
    "\n",
    "    token_count: int = 0\n",
    "    n = max_tokens or float(\"inf\")\n",
    "    page_content: list[str] = []\n",
    "    metadata: list[Metadata | None] = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if token_count + chunk.token_count >= n:\n",
    "            document = Document(\n",
    "                page_content=\" \".join(page_content),\n",
    "                metadata=reduce(Metadata.merge, metadata),\n",
    "            )\n",
    "            documents.append(document)\n",
    "            token_count = 0\n",
    "            page_content = []\n",
    "            metadata = []\n",
    "\n",
    "        page_content.append(chunk.text)\n",
    "        metadata.append(chunk.metadata)\n",
    "        token_count += chunk.token_count\n",
    "\n",
    "    if len(page_content) > 0:\n",
    "        document = Document(\n",
    "            page_content=\" \".join(page_content),\n",
    "            metadata=reduce(Metadata.merge, metadata),\n",
    "        )\n",
    "        documents.append(document)\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise(\n",
    "    chat_request: ChatRequest,\n",
    "    file_uuid: UUID,\n",
    "    user_uuid: UUID,\n",
    "    llm: ChatLiteLLM,\n",
    "    storage_handler: ElasticsearchStorageHandler,\n",
    ") -> ChatResponse:\n",
    "    question = chat_request.message_history[-1].text\n",
    "    previous_history = list(chat_request.message_history[:-1])\n",
    "    \n",
    "    # get full doc from vector store\n",
    "    documents = get_file_as_documents(\n",
    "        file_uuid=file_uuid, \n",
    "        user_uuid=user_uuid, \n",
    "        storage_handler=storage_handler,\n",
    "        max_tokens=20_000\n",
    "    )\n",
    "    \n",
    "    # right now, can only handle a single document so we manually truncate\n",
    "    document = documents[:1]\n",
    "    if len(documents) > 1:\n",
    "        print(\"Document was longer than 20k tokens. Truncating to the first 20k.\")\n",
    "    \n",
    "    # stuff raw prompt\n",
    "    chat_history = [\n",
    "        (\"system\", _core_redbox_prompt),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "        (\"user\", \"Question: {question}. \\n\\n Content: \\n\\n<document> {content} </document> \\n\\n Answer: \"),\n",
    "    ]\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    chain = (\n",
    "        {\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"messages\": itemgetter(\"messages\"),\n",
    "            \"content\": itemgetter(\"content\") | RunnableLambda(format_docs),\n",
    "        }\n",
    "        | ChatPromptTemplate.from_messages(chat_history)\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # return\n",
    "    return chain.invoke(\n",
    "        input={\n",
    "            \"question\": question,\n",
    "            \"content\": document,\n",
    "            \"messages\": [(msg.role, msg.text) for msg in previous_history]\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "chat_request_body = {\n",
    "    \"message_history\": [\n",
    "        # {\"text\": \"Can you always refer to BEIS as the Department for Business, Energy and Industrial Strategy from now on?\", \"role\": \"user\"},\n",
    "        # {\"text\": \"Of course. In future responses I will always expand the BEIS acronym.\", \"role\": \"ai\"},\n",
    "        {\"text\": \"Please summarise all the key people in this document and who they work for.\", \"role\": \"user\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "res = summarise(\n",
    "    chat_request=ChatRequest(**chat_request_body),\n",
    "    file_uuid=UUID(\"35b3d95f-7f65-4cae-b159-22001ca19c88\"),\n",
    "    user_uuid=UUID(\"b92ebddb-a77e-4ed7-81b9-a2f7ce814ef5\"),\n",
    "    llm=llm,\n",
    "    storage_handler=storage_handler\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-Vh_-Fb0j-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
