{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Evaluation for Redbox RAG chat  <a class=\"anchor\" id=\"title\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a class=\"anchor\" id=\"toc\"></a>\n",
    "* [Overview](#one-section)\n",
    "* [Metrics](#two-section)\n",
    "    - [Fathfulness]()\n",
    "    - [Answer Relevancy]()\n",
    "    - [Hallucination]()\n",
    "* [Evaluation Dataset](#three-section)\n",
    "* [Evaluation Workflow in this Notebook](#four-section)\n",
    "* [Prompt Playground](#five-section)\n",
    "    - [RAG prompts](#six-section)\n",
    "* [Generate RAG responses and append them to evaluation dataset](#seven-section)\n",
    "\n",
    "* [eight](#eight-section)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview <a class=\"anchor\" id=\"one-section\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to optimising the generation part of our RAG system, the only thing that we can modify are the `RAG prompts` that are passed with context to the LLM. Other components certainly play into the overall generation evaluation score, such as is the retrieved context of high-quality, but the levers to change these other components are further upstream in the RAG pipeline, and evaluated in Retrieval Evaluation and e2d Evaluation notebooks. These other components are also slower to change compared to prompts, which are just natural language!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to avoid using the /chat/rag endpoint for quick experimentation with `RAG prompts`, as the need to rebuild the core_api docker image, start and stop container etc will really slow down development --> changing prompts is very quick to do, so we want quick evaluation of how these prompt changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this reason, the /chat/rag endpoint function is in this notebook, and prompts can be changed in a single place, followed by much quicker feedback. If your prompt experiments look good, i.e. they improve generation evalution metrics, then you can consider making these changes in the `core_api` service. Information on where to make the corresponding changesin the the `core_api` service are at the bottom of this notebook. Once you make changes in `core_api` and rebuild, these changes will be reflected in the deployed /chat/rag endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate RAG generation using metrics described in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics <a class=\"anchor\" id=\"two-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by using 3 DeepEval metrics:\n",
    "- Faithfulness\n",
    "- Answer Relevancy **(what are we taking as 'input'? Raw question or refined question?)**\n",
    "- Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness\n",
    "\n",
    "The faithfulness metric measures the quality of your RAG pipeline's generator by evaluating whether the `actual_output` factually aligns with the contents of your `retrieval_context`. `deepeval`'s faithfulness metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Arguments\n",
    "To use the `FaithfulnessMetric`, you need to provide the following arguments when creating an LLMTestCase:\n",
    "\n",
    "- `input`\n",
    "- `actual_output`\n",
    "- `retrieval_context`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Relevancy\n",
    "The answer relevancy metric measures the quality of your RAG pipeline's generator by evaluating how relevant the actual_output of your LLM application is compared to the provided `input`. `deepeval`'s answer relevancy metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Arguments\n",
    "To use the AnswerRelevancyMetric, you'll have to provide the following arguments when creating an LLMTestCase:\n",
    "\n",
    "- `input`\n",
    "- `actual_output`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucination\n",
    "The hallucination metric determines whether your LLM generates factually correct information by comparing the `actual_output` to the provided `context`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Arguments\n",
    "To use the HallucinationMetric, you'll have to provide the following arguments when creating an LLMTestCase:\n",
    "\n",
    "- `input`\n",
    "- `actual_output`\n",
    "- `retrieval_context`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Redbox locally\n",
    "We want to take advantage of the document processing part of the redbox `file` api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jose import jwt\n",
    "from uuid import UUID\n",
    "\n",
    "bearer_token = jwt.encode({\"user_uuid\": str(UUID(\"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\"))}, key=\"your-secret-key\", algorithm=\"HS512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJ1c2VyX3V1aWQiOiJhYWFhYWFhYS1hYWFhLWFhYWEtYWFhYS1hYWFhYWFhYWFhYWEifQ.kwzm-8i8SveeqYqvsRUm4FiB7nd3I43aI70ImljgdudKM4xrDw9z3CUpEBRwqqh6D3ZghB2T-Lu7BlV36VR5sg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bearer_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Do the manual steps below programmatically, from 2 onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "user_uuid = uuid.uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://127.0.0.1:5002/file/upload'\n",
    "\n",
    "files = {\n",
    "    'file': open('Universal-Basic-Income-Scotland-Report.pdf', 'rb'),  # replace with your actual file path\n",
    "}\n",
    "\n",
    "response = requests.post(url, files=files, data=data, headers={\"Authorization\": f\"Bearer {bearer_token}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not parse response as JSON:  Internal Server Error\n"
     ]
    }
   ],
   "source": [
    "# Check if the response is not empty\n",
    "if response.text:\n",
    "    # Try to parse the response as JSON\n",
    "    try:\n",
    "        print(response.json())\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Could not parse response as JSON: \", response.text)\n",
    "else:\n",
    "    print(\"Empty response from server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://127.0.0.1:5002/file/upload'\n",
    "\n",
    "files = {\n",
    "    'file': open('Universal-Basic-Income-Scotland-Report.pdf', 'rb'),  # replace with your actual file path\n",
    "}\n",
    "data = {\"user_uuid\": f\"{user_uuid}\"}\n",
    "\n",
    "response = requests.post(url, files=files, data=data, headers={\"Authorization\": f\"Bearer {bearer_token}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not parse response as JSON:  Internal Server Error\n"
     ]
    }
   ],
   "source": [
    "# Check if the response is not empty\n",
    "if response.text:\n",
    "    # Try to parse the response as JSON\n",
    "    try:\n",
    "        print(response.json())\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Could not parse response as JSON: \", response.text)\n",
    "else:\n",
    "    print(\"Empty response from server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code:  403\n"
     ]
    }
   ],
   "source": [
    "# Inspect response content\n",
    "print(\"Status code: \", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response content:  {\"detail\":\"Not authenticated\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Response content: \", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/file endpoint (from Will)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.BufferedReader' object has no attribute 'filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:5002/file/upload\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     json\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m----> 4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m,\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbucket\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39mbucket_name,\n\u001b[1;32m      6\u001b[0m     },\n\u001b[1;32m      7\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: bearer_token},\n\u001b[1;32m      8\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_io.BufferedReader' object has no attribute 'filename'"
     ]
    }
   ],
   "source": [
    "response = requests.post(\n",
    "    'http://127.0.0.1:5002/file/upload',\n",
    "    json={\n",
    "        \"key\": file.filename,\n",
    "        \"bucket\": self._settings.bucket_name,\n",
    "    },\n",
    "    headers={\"Authorization\": bearer_token},\n",
    "    timeout=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not parse response as JSON:  Internal Server Error\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'http://127.0.0.1:5002/file/upload'\n",
    "headers = {\n",
    "    'Authorization': f\"Bearer {bearer_token}\",\n",
    "    'user_uuid': f\"{user_uuid}\",\n",
    "    \n",
    "}\n",
    "files = {\n",
    "    'file': open('Universal-Basic-Income-Scotland-Report.pdf', 'rb'),  # replace with your actual file path\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, files=files)\n",
    "\n",
    "# Check if the response is not empty\n",
    "if response.text:\n",
    "    # Try to parse the response as JSON\n",
    "    try:\n",
    "        print(response.json())\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Could not parse response as JSON: \", response.text)\n",
    "else:\n",
    "    print(\"Empty response from server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('Universal-Basic-Income-Scotland-Report.pdf', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/data_eval/Universal-Basic-Income-Scotland-Report.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " @file_app.post(\"/upload\", tags=[\"file\"], response_model=File)\n",
    "    async def upload_file(user_uuid: Annotated[UUID, Depends(get_user_uuid)], file: UploadFile = None) -> File:\n",
    "        \"\"\"Upload a file to the object store\n",
    "\n",
    "        Args:\n",
    "            file (UploadFile): The file to upload\n",
    "\n",
    "        Returns:\n",
    "            File: The file that was uploaded\n",
    "        \"\"\"\n",
    "        file = file or FastAPIFile(...)\n",
    "        key = file.filename\n",
    "        s3.upload_fileobj(file.file, env.bucket_name, key)\n",
    "\n",
    "        file = File(key=key, bucket=env.bucket_name, creator_user_uuid=user_uuid)\n",
    "        storage_handler.write_item(file)\n",
    "\n",
    "        log.info(\"publishing %s\", file.uuid)\n",
    "        await file_publisher.publish(file)\n",
    "\n",
    "        return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    ...\n",
    "    headers={\"Authorization\": f\"Bearer {bearer_token}\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the steps below to get the chunks for the evaluation document(s):\n",
    "\n",
    "1. Run app locally WITHOUT detached mode: `docker compose up elasticsearch kibana worker minio redis core-api db`\n",
    "\n",
    "2. View Swagger UI for /file endpoint at: `http://127.0.0.1:5002/file/docs`\n",
    "\n",
    "3. Authorise yourself. Top right of Swagger docs, click on Authorise button. Paste in the `bearer_token` generated in the call above.\n",
    "\n",
    "4. Upload documents selected for evaluation\n",
    "\n",
    "5. Take a note of the uuid(s), e.g. 7b550232-35c4-48fd-8d7a-ba364c1378c4 (this will change each time you run locally)\n",
    "\n",
    "Chunking happens very quickly. Embedding takes more time, but will give you a boolean flag on complete.\n",
    "\n",
    "6. From the Swagger UI, use the `file/{uuid}/status` endpoint to check status. Use the `uuid`s noted in step 4\n",
    "\n",
    "7. From the Swagger UI use the `{file_uuid}/chunks` endpoint to get the chunks required for the next step of evaluation. Use the `uuid`s noted in step 4 to get chunks required for evaluation.\n",
    "\n",
    "The complete output can be downloaded in JSON format from the Swagger UI docs page\n",
    "\n",
    "8. Move downloaded response into `notebooks/evaluation/data_eval` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup <a class=\"anchor\" id=\"five-section\"></a>\n",
    "\n",
    "Some basic setup for RAG chat function to work during experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Pydantic chat models, used by the RAG chat function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redbox.models.chat import ChatMessage, ChatRequest, ChatResponse, SourceDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mock HTTP Authorization Credentials, used by the RAG chat function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core_api.src.auth import get_user_uuid\n",
    "from fastapi.security import HTTPAuthorizationCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock HTTP Authorization Credentials\n",
    "credentials = Mock(spec=HTTPAuthorizationCredentials)\n",
    "credentials.credentials = \"mock_token\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAG chat function imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Dataset <a class=\"anchor\" id=\"three-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Load evaluation dataset for generation evaluation\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "dataset = EvaluationDataset()\n",
    "dataset.add_test_cases_from_json_file(\n",
    "    # file_path is the absolute path to you .json file\n",
    "    file_path=\"example.json\",\n",
    "    input_key_name=\"query\",\n",
    "    actual_output_key_name=\"actual_output\",\n",
    "    expected_output_key_name=\"expected_output\",\n",
    "    context_key_name=\"context\",\n",
    "    retrieval_context_key_name=\"retrieval_context\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Workflow in this Notebook <a class=\"anchor\" id=\"four-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow these steps to run an experiment:\n",
    "1. Make experimental changes to [`RAG prompts`]() - these will be used by the /chat/rag function\n",
    "2. (Optional) Make experimental changes to the [/chat/rag function]() \n",
    "3. Pass the evaluation dataset through the /chat/rag function to general `actual_output` and append these to the evaluation dataset\n",
    "4. Run evaluations on dataset to calcuate generation evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Playground <a class=\"anchor\" id=\"five-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do an initial run through this notebook with the starting/default prompts BEFORE your first experiment.** This will give you baseline scores for each metric to compare your experiment results against.\n",
    "\n",
    "Add baseline scores below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline evaluation\n",
    "\n",
    "**[2024-05-15] Baseline scores**\n",
    "Using the deployed /chat/rag enpoint to get `actual_output` from Redbox RAG chat, we got the following baseline scores for each metric:\n",
    "- Faithfulness: **#TODO: Populate after first run through**\n",
    "- Answer Relevancy: **#TODO: Populate after first run through**\n",
    "- Hallucination: **#TODO: Populate after first run through**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have done your first run through the notebook, please experiment with these prompts as you see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to experiment with:\n",
    "1. `_core_redbox_prompt`\n",
    "2. `CORE_REDBOX_PROMPT`\n",
    "3. `_with_sources_template`\n",
    "4. `WITH_SOURCES_PROMPT`\n",
    "5. `_stuff_document_template`\n",
    "6. `STUFF_DOCUMENT_PROMPT`\n",
    "7. The LLM being used - **For now, please stick with gpt-3.5-turbo, as we establish a baseline quality**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG prompts  <a class=\"anchor\" id=\"six-section\"></a>\n",
    "These are the prompts that will have most effect on RAG generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_core_redbox_prompt = \"\"\"You are RedBox Copilot. An AI focused on helping UK Civil Servants, Political Advisors and\\\n",
    "Ministers triage and summarise information from a wide variety of sources. You are impartial and\\\n",
    "non-partisan. You are not a replacement for human judgement, but you can help humans\\\n",
    "make more informed decisions. If you are asked a question you cannot answer based on your following instructions, you\\\n",
    "should say so. Be concise and professional in your responses. Respond in markdown format.\n",
    "\n",
    "=== RULES ===\n",
    "\n",
    "All responses to Tasks **MUST** be translated into the user's preferred language.\\\n",
    "This is so that the user can understand your responses.\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where CORE_REDBOX_PROMPT is used in the codebase\n",
    "CORE_REDBOX_PROMPT = PromptTemplate.from_template(_core_redbox_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_with_sources_template = \"\"\"Given the following extracted parts of a long document and \\\n",
    "a question, create a final answer with Sources at the end.  \\\n",
    "If you don't know the answer, just say that you don't know. Don't try to make \\\n",
    "up an answer.\n",
    "Be concise in your response and summarise where appropriate. \\\n",
    "At the end of your response add a \"Sources:\" section with the documents you used. \\\n",
    "DO NOT reference the source documents in your response. Only cite at the end. \\\n",
    "ONLY PUT CITED DOCUMENTS IN THE \"Sources:\" SECTION AND NO WHERE ELSE IN YOUR RESPONSE. \\\n",
    "IT IS CRUCIAL that citations only happens in the \"Sources:\" section. \\\n",
    "This format should be <DocX> where X is the document UUID being cited.  \\\n",
    "DO NOT INCLUDE ANY DOCUMENTS IN THE \"Sources:\" THAT YOU DID NOT USE IN YOUR RESPONSE. \\\n",
    "YOU MUST CITE USING THE <DocX> FORMAT. NO OTHER FORMAT WILL BE ACCEPTED.\n",
    "Example: \"Sources: <DocX> <DocY> <DocZ>\"\n",
    "\n",
    "Use **bold** to highlight the most question relevant parts in your response.\n",
    "If dealing dealing with lots of data return it in markdown table format.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "FINAL ANSWER:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WITH_SOURCES_PROMPT = PromptTemplate.from_template(_core_redbox_prompt + _with_sources_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stuff_document_template = \"<Doc{parent_doc_uuid}>{page_content}</Doc{parent_doc_uuid}>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUFF_DOCUMENT_PROMPT = PromptTemplate.from_template(_stuff_document_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find changes to the prompts above improve the generation evaluation scores, please consider making a PR to update the code in main.\n",
    "\n",
    "All these prompts are locations in [chat.py](../../redbox/llm/prompts/chat.py), except `_core_redbox_prompt` which is located in [core.py](../../redbox/llm/prompts/core.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate RAG responses and append them to evaluation dataset  <a class=\"anchor\" id=\"seven-section\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: Load required functions\n",
    "\n",
    "#TODO: Any functions below that we need to mock?\n",
    "\n",
    "# I would like to keep the rag_chat function unchanged from what it is in the core_api repo.\n",
    "\n",
    "# However, the user_uuid is only used for authorisation (it is NOT used for authentication), so if too troublesome, can be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create `ChatRequest` for evaluation dataset, used by the RAG chat function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `ChatRequest` Pydantic model used by the RAG chat function, the JSON body should contain a `message_history` key with a list of chat messages.\n",
    "\n",
    "Each chat message should match the structure defined by the `ChatMessage` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_request = {\n",
    "                \"message_history\": [\n",
    "                        {\"text\": \"You are a helpful AI Assistant\", \"role\": \"system\"},\n",
    "                        {\"text\": \"What is AI?\", \"role\": \"user\"},\n",
    "                ]\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_history': [{'text': 'You are a helpful AI Assistant',\n",
       "   'role': 'system'},\n",
       "  {'text': 'What is AI?', 'role': 'user'}]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is AI?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This only works within the function (or FastAPI), due to attribute access: question = chat_request.message_history[-1].text\n",
    "question = chat_request[\"message_history\"][-1]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAG chat function - generation part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLM setup ===\n",
    "llm = ChatLiteLLM(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat(chat_request: ChatRequest, user_uuid: Annotated[UUID, Depends(get_user_uuid)]) -> ChatResponse:\n",
    "    \"\"\"Get a LLM response to a question history and file\n",
    "\n",
    "    Args:\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        StreamingResponse: a stream of the chain response\n",
    "    \"\"\"\n",
    "    # question = chat_request.message_history[-1].text\n",
    "    # previous_history = list(chat_request.message_history[:-1])\n",
    "    # previous_history = ChatPromptTemplate.from_messages(\n",
    "    #     (msg.role, msg.text) for msg in previous_history\n",
    "    # ).format_messages()\n",
    "\n",
    "    # condense_question_chain = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "    # standalone_question = condense_question_chain({\"question\": question, \"chat_history\": previous_history})[\"text\"]\n",
    "\n",
    "    # docs = vector_store.as_retriever(\n",
    "    #     search_kwargs={\"filter\": {\"term\": {\"creator_user_uuid.keyword\": str(user_uuid)}}}\n",
    "    # ).get_relevant_documents(standalone_question)\n",
    "\n",
    "    docs_with_sources_chain = load_qa_with_sources_chain(\n",
    "        llm,\n",
    "        chain_type=\"stuff\",\n",
    "        prompt=WITH_SOURCES_PROMPT,\n",
    "        document_prompt=STUFF_DOCUMENT_PROMPT,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    result = docs_with_sources_chain(\n",
    "        {\n",
    "            \"question\": standalone_question,\n",
    "            \"input_documents\": docs,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    source_documents = [\n",
    "        SourceDocument(\n",
    "            page_content=langchain_document.page_content,\n",
    "            file_uuid=langchain_document.metadata.get(\"parent_doc_uuid\"),\n",
    "            page_numbers=langchain_document.metadata.get(\"page_numbers\"),\n",
    "        )\n",
    "        for langchain_document in result.get(\"input_documents\", [])\n",
    "    ]\n",
    "    return ChatResponse(output_text=result[\"output_text\"], source_documents=source_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate `actual_output` using RAG and evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: This is where we put it all together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append `actual_output` to evaluation dataset\n",
    "Process the goldens and convert them into test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A hypothetical LLM application example\n",
    "from chatbot import query\n",
    "from typing import List\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import Golden\n",
    "...\n",
    "\n",
    "def convert_goldens_to_test_cases(goldens: List[Golden]) -> List[LLMTestCase]:\n",
    "    test_cases = []\n",
    "    for golden in goldens:\n",
    "        test_case = LLMTestCase(\n",
    "            input=golden.input,\n",
    "            # Generate actual output using the 'input' and 'additional_metadata'\n",
    "            actual_output = rag_chat(chat_request: ChatRequest, user_uuid=1234)\n",
    "            actual_output = rag_chat\n",
    "            actual_output=query(golden.input, golden.additional_metadata),\n",
    "            expected_output=golden.expected_output,\n",
    "            context=golden.context,\n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "    return test_cases\n",
    "\n",
    "# Data preprocessing before setting the dataset test cases\n",
    "dataset.test_cases = convert_goldens_to_test_cases(dataset.goldens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run generation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andy/Library/Caches/pypoetry/virtualenvs/redbox-MiicHf1r-py3.11/lib/python3.11/site-packages/deepeval/__init__.py:42: UserWarning: You are using deepeval version 0.21.36, however version 0.21.42 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promote optimised prompts into production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find changes to the prompts above improve the generation evaluation scores, please consider making a PR to update the code in `core_api`. Follow these steps:\n",
    "\n",
    "1. Create a new branch off `main`\n",
    "2. Make changes in the locations listed below\n",
    "3. Run through the e2e RAG evaluation notebook\n",
    "4. If e2e RAG evaluation metrics are improved, please make a PR!\n",
    "\n",
    "All these prompts are locations in [chat.py](../../redbox/llm/prompts/chat.py), except `_core_redbox_prompt` which is located in [core.py](../../redbox/llm/prompts/core.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-MiicHf1r-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
