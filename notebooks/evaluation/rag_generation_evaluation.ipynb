{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Evaluation for Redbox RAG chat  <a class=\"anchor\" id=\"title\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a class=\"anchor\" id=\"toc\"></a>\n",
    "* [Overview](#one-section)\n",
    "* [Metrics](#two-section)\n",
    "    - [Fathfulness]()\n",
    "    - [Answer Relevancy]()\n",
    "    - [Hallucination]()\n",
    "* [Evaluation Dataset](#three-section)\n",
    "* [Evaluation Workflow in this Notebook](#four-section)\n",
    "* [Prompt Playground](#five-section)\n",
    "    - [RAG prompts](#six-section)\n",
    "* [Generate RAG responses and append them to evaluation dataset](#seven-section)\n",
    "\n",
    "* [eight](#eight-section)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview <a class=\"anchor\" id=\"one-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to optimising the generation part of our RAG system, the only thing that we can modify are the `RAG prompts` that are passed with context to the LLM. Other components certainly play into the overall generation evaluation score, such as is the retrieved context of high-quality, but the levers to change these other components are further upstream in the RAG pipeline, and evaluated in Retrieval Evaluation and e2d Evaluation notebooks. These other components are also slower to change compared to prompts, which are just natural language!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to avoid using the /chat/rag endpoint for quick experimentation with `RAG prompts`, as the need to rebuild the core_api docker image, start and stop container etc will really slow down development --> changing prompts is very quick to do, so we want quick evaluation of how these prompt changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this reason, the /chat/rag endpoint function is in this notebook, and prompts can be changed in a single place, followed by much quicker feedback. If your prompt experiments look good, i.e. they improve generation evalution metrics, then you can consider making these changes in the `core_api` service. Information on where to make the corresponding changesin the the `core_api` service are at the bottom of this notebook. Once you make changes in `core_api` and rebuild, these changes will be reflected in the deployed /chat/rag endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate RAG generation using metrics described in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics <a class=\"anchor\" id=\"two-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by using 3 DeepEval metrics:\n",
    "- Faithfulness\n",
    "- Answer Relevancy **(what are we taking as 'input'? Raw question or refined question?)**\n",
    "- Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness\n",
    "\n",
    "The faithfulness metric measures the quality of your RAG pipeline's generator by evaluating whether the `actual_output` factually aligns with the contents of your `retrieval_context`. `deepeval`'s faithfulness metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Arguments\n",
    "To use the `FaithfulnessMetric`, you need to provide the following arguments when creating an LLMTestCase:\n",
    "\n",
    "- `input`\n",
    "- `actual_output`\n",
    "- `retrieval_context`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Relevancy\n",
    "The answer relevancy metric measures the quality of your RAG pipeline's generator by evaluating how relevant the actual_output of your LLM application is compared to the provided `input`. `deepeval`'s answer relevancy metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Arguments\n",
    "To use the AnswerRelevancyMetric, you'll have to provide the following arguments when creating an LLMTestCase:\n",
    "\n",
    "- `input`\n",
    "- `actual_output`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucination\n",
    "The hallucination metric determines whether your LLM generates factually correct information by comparing the `actual_output` to the provided `context`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Arguments\n",
    "To use the HallucinationMetric, you'll have to provide the following arguments when creating an LLMTestCase:\n",
    "\n",
    "- `input`\n",
    "- `actual_output`\n",
    "- `retrieval_context`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Dataset <a class=\"anchor\" id=\"three-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Load evaluation dataset for generation evaluation\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "dataset = EvaluationDataset()\n",
    "dataset.add_test_cases_from_json_file(\n",
    "    # file_path is the absolute path to you .json file\n",
    "    file_path=\"example.json\",\n",
    "    input_key_name=\"query\",\n",
    "    actual_output_key_name=\"actual_output\",\n",
    "    expected_output_key_name=\"expected_output\",\n",
    "    context_key_name=\"context\",\n",
    "    retrieval_context_key_name=\"retrieval_context\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Workflow in this Notebook <a class=\"anchor\" id=\"four-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow these steps to run an experiment:\n",
    "1. Make experimental changes to [`RAG prompts`]() - these will be used by the /chat/rag function\n",
    "2. (Optional) Make experimental changes to the [/chat/rag function]() \n",
    "3. Pass the evaluation dataset through the /chat/rag function to general `actual_output` and append these to the evaluation dataset\n",
    "4. Run evaluations on dataset to calcuate generation evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Playground <a class=\"anchor\" id=\"five-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do an initial run through this notebook with the starting/default prompts BEFORE your first experiment.** This will give you baseline scores for each metric to compare your experiment results against.\n",
    "\n",
    "Add baseline scores below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline evaluation\n",
    "\n",
    "**[2024-05-15] Baseline scores**\n",
    "Using the deployed /chat/rag enpoint to get `actual_output` from Redbox RAG chat, we got the following baseline scores for each metric:\n",
    "- Faithfulness: **#TODO: Populate after first run through**\n",
    "- Answer Relevancy: **#TODO: Populate after first run through**\n",
    "- Hallucination: **#TODO: Populate after first run through**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have done your first run through the notebook, please experiment with these prompts as you see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to experiment with:\n",
    "1. _chat_template\n",
    "2. CONDENSE_QUESTION_PROMPT\n",
    "3. _core_redbox_prompt\n",
    "4. CORE_REDBOX_PROMPT\n",
    "5. _with_sources_template\n",
    "6. WITH_SOURCES_PROMPT\n",
    "7. _stuff_document_template\n",
    "8. STUFF_DOCUMENT_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refining Question Prompts\n",
    "As this refining of the question is pre-retrieval, should we **remove it** from this generation evalution notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_chat_template = \"\"\"Given the following conversation and a follow up question,\n",
    "rephrase the follow up question to be a standalone question, in its original\n",
    "language. include the follow up instructions in the standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG prompts  <a class=\"anchor\" id=\"six-section\"></a>\n",
    "These are the prompts that will have most effect on RAG generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_core_redbox_prompt = \"\"\"You are RedBox Copilot. An AI focused on helping UK Civil Servants, Political Advisors and\\\n",
    "Ministers triage and summarise information from a wide variety of sources. You are impartial and\\\n",
    "non-partisan. You are not a replacement for human judgement, but you can help humans\\\n",
    "make more informed decisions. If you are asked a question you cannot answer based on your following instructions, you\\\n",
    "should say so. Be concise and professional in your responses. Respond in markdown format.\n",
    "\n",
    "=== RULES ===\n",
    "\n",
    "All responses to Tasks **MUST** be translated into the user's preferred language.\\\n",
    "This is so that the user can understand your responses.\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where CORE_REDBOX_PROMPT is used in the codebase\n",
    "CORE_REDBOX_PROMPT = PromptTemplate.from_template(_core_redbox_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_with_sources_template = \"\"\"Given the following extracted parts of a long document and \\\n",
    "a question, create a final answer with Sources at the end.  \\\n",
    "If you don't know the answer, just say that you don't know. Don't try to make \\\n",
    "up an answer.\n",
    "Be concise in your response and summarise where appropriate. \\\n",
    "At the end of your response add a \"Sources:\" section with the documents you used. \\\n",
    "DO NOT reference the source documents in your response. Only cite at the end. \\\n",
    "ONLY PUT CITED DOCUMENTS IN THE \"Sources:\" SECTION AND NO WHERE ELSE IN YOUR RESPONSE. \\\n",
    "IT IS CRUCIAL that citations only happens in the \"Sources:\" section. \\\n",
    "This format should be <DocX> where X is the document UUID being cited.  \\\n",
    "DO NOT INCLUDE ANY DOCUMENTS IN THE \"Sources:\" THAT YOU DID NOT USE IN YOUR RESPONSE. \\\n",
    "YOU MUST CITE USING THE <DocX> FORMAT. NO OTHER FORMAT WILL BE ACCEPTED.\n",
    "Example: \"Sources: <DocX> <DocY> <DocZ>\"\n",
    "\n",
    "Use **bold** to highlight the most question relevant parts in your response.\n",
    "If dealing dealing with lots of data return it in markdown table format.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "FINAL ANSWER:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WITH_SOURCES_PROMPT = PromptTemplate.from_template(_core_redbox_prompt + _with_sources_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stuff_document_template = \"<Doc{parent_doc_uuid}>{page_content}</Doc{parent_doc_uuid}>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUFF_DOCUMENT_PROMPT = PromptTemplate.from_template(_stuff_document_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find changes to the prompts above improve the generation evaluation scores, please consider making a PR to update the code in main.\n",
    "\n",
    "All these prompts are locations in [chat.py](../../redbox/llm/prompts/chat.py), except `_core_redbox_prompt` which is located in [core.py](../../redbox/llm/prompts/core.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate RAG responses and append them to evaluation dataset  <a class=\"anchor\" id=\"seven-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set everything up to run RAG chat from this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: Load required functions\n",
    "\n",
    "#TODO: Any functions below that we need to mock?\n",
    "\n",
    "# I would like to keep the rag_chat function unchanged from what it is in the core_api repo.\n",
    "\n",
    "# However, the user_uuid is only used for authorisation (it is NOT used for authentication), so if too troublesome, can be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Pydantic chat models, used by the RAG chat function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redbox.models.chat import ChatMessage, ChatRequest, ChatResponse, SourceDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create `ChatRequest` for evaluation dataset, used by the RAG chat function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `ChatRequest` Pydantic model used by the RAG chat function, the JSON body should contain a `message_history` key with a list of chat messages.\n",
    "\n",
    "Each chat message should match the structure defined by the `ChatMessage` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_request = {\n",
    "                \"message_history\": [\n",
    "                        {\"text\": \"You are a helpful AI Assistant\", \"role\": \"system\"},\n",
    "                        {\"text\": \"What is AI?\", \"role\": \"user\"},\n",
    "                ]\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_history': [{'text': 'You are a helpful AI Assistant',\n",
       "   'role': 'system'},\n",
       "  {'text': 'What is AI?', 'role': 'user'}]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is AI?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This only works within the function (or FastAPI), due to attribute access: question = chat_request.message_history[-1].text\n",
    "question = chat_request[\"message_history\"][-1]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mock HTTP Authorization Credentials, used by the RAG chat function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from core_api.src.auth import get_user_uuid\n",
    "from fastapi.security import HTTPAuthorizationCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock HTTP Authorization Credentials\n",
    "credentials = Mock(spec=HTTPAuthorizationCredentials)\n",
    "credentials.credentials = \"mock_token\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAG chat function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LLM setup ===\n",
    "llm = ChatLiteLLM(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat(chat_request: ChatRequest, user_uuid: Annotated[UUID, Depends(get_user_uuid)]) -> ChatResponse:\n",
    "    \"\"\"Get a LLM response to a question history and file\n",
    "\n",
    "    Args:\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        StreamingResponse: a stream of the chain response\n",
    "    \"\"\"\n",
    "    # question = chat_request.message_history[-1].text\n",
    "    # previous_history = list(chat_request.message_history[:-1])\n",
    "    # previous_history = ChatPromptTemplate.from_messages(\n",
    "    #     (msg.role, msg.text) for msg in previous_history\n",
    "    # ).format_messages()\n",
    "\n",
    "    # condense_question_chain = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "    # standalone_question = condense_question_chain({\"question\": question, \"chat_history\": previous_history})[\"text\"]\n",
    "\n",
    "    # docs = vector_store.as_retriever(\n",
    "    #     search_kwargs={\"filter\": {\"term\": {\"creator_user_uuid.keyword\": str(user_uuid)}}}\n",
    "    # ).get_relevant_documents(standalone_question)\n",
    "\n",
    "    docs_with_sources_chain = load_qa_with_sources_chain(\n",
    "        llm,\n",
    "        chain_type=\"stuff\",\n",
    "        prompt=WITH_SOURCES_PROMPT,\n",
    "        document_prompt=STUFF_DOCUMENT_PROMPT,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    result = docs_with_sources_chain(\n",
    "        {\n",
    "            \"question\": standalone_question,\n",
    "            \"input_documents\": docs,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    source_documents = [\n",
    "        SourceDocument(\n",
    "            page_content=langchain_document.page_content,\n",
    "            file_uuid=langchain_document.metadata.get(\"parent_doc_uuid\"),\n",
    "            page_numbers=langchain_document.metadata.get(\"page_numbers\"),\n",
    "        )\n",
    "        for langchain_document in result.get(\"input_documents\", [])\n",
    "    ]\n",
    "    return ChatResponse(output_text=result[\"output_text\"], source_documents=source_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate `actual_output` using RAG and evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: This is where we put it all together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append `actual_output` to evaluation dataset\n",
    "Process the goldens and convert them into test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A hypothetical LLM application example\n",
    "from chatbot import query\n",
    "from typing import List\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import Golden\n",
    "...\n",
    "\n",
    "def convert_goldens_to_test_cases(goldens: List[Golden]) -> List[LLMTestCase]:\n",
    "    test_cases = []\n",
    "    for golden in goldens:\n",
    "        test_case = LLMTestCase(\n",
    "            input=golden.input,\n",
    "            # Generate actual output using the 'input' and 'additional_metadata'\n",
    "            actual_output = rag_chat(chat_request: ChatRequest, user_uuid=1234)\n",
    "            actual_output = rag_chat\n",
    "            actual_output=query(golden.input, golden.additional_metadata),\n",
    "            expected_output=golden.expected_output,\n",
    "            context=golden.context,\n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "    return test_cases\n",
    "\n",
    "# Data preprocessing before setting the dataset test cases\n",
    "dataset.test_cases = convert_goldens_to_test_cases(dataset.goldens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run generation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andy/Library/Caches/pypoetry/virtualenvs/redbox-MiicHf1r-py3.11/lib/python3.11/site-packages/deepeval/__init__.py:42: UserWarning: You are using deepeval version 0.21.36, however version 0.21.42 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-MiicHf1r-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
