{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Evaluation for Redbox RAG  <a class=\"anchor\" id=\"title\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a class=\"anchor\" id=\"toc\"></a>\n",
    "* [Overview](#one-section)\n",
    "* [Metrics](#two-section)\n",
    "    - [Fathfulness]()\n",
    "    - [Answer Relevancy]()\n",
    "    - [Hallucination]()\n",
    "* [Evaluation Dataset](#three-section)\n",
    "* [Prompt Playground](#four-section)\n",
    "* [Generate `actual_output` using RAG and evaluation dataset](#five-section)\n",
    "* [six](#six-section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview <a class=\"anchor\" id=\"one-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to optimising the generation part of our RAG system, the only thing that we can modify are the `RAG prompts` that are passed with context to the LLM. Certainly, other things play into the overall generation evaluation score, such as is the retrieved context of high-quality, but the levers to change this are further upstream in the RAG pipeline, and evaluated in Retrieval Evaluation and e2d Evaluation notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to avoid using the /chat/rag endpoint for quick experimentation with `RAG prompts`, as the need to rebuild docker image, start and stop container etc will really slow down development --> changing prompts is very quick to do, so we want quick evaluation of how these prompt changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this reason, the /chat/rag endpoint function is in this notebook, and prompts can be changed in a single place, followed by much quicker feedback. If the prompt experiments you do look good, there is information at the bottom of this notebook about where you should make the corresponding changes in the core-api to have these changes reflected in the deployed /chat/rag endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate RAG generation using metrics described in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics <a class=\"anchor\" id=\"two-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by using 3 DeepEval metrics:\n",
    "- Faithfulness\n",
    "- Answer Relevancy **(what are we taking as 'input'? Raw question or refined question?)**\n",
    "- Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness\n",
    "\n",
    "The faithfulness metric measures the quality of your RAG pipeline's generator by evaluating whether the `actual_output` factually aligns with the contents of your `retrieval_context`. `deepeval`'s faithfulness metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Arguments\n",
    "To use the `FaithfulnessMetric`, you need to provide the following arguments when creating an LLMTestCase:\n",
    "\n",
    "- `input`\n",
    "- `actual_output`\n",
    "- `retrieval_context`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Relevancy\n",
    "The answer relevancy metric measures the quality of your RAG pipeline's generator by evaluating how relevant the actual_output of your LLM application is compared to the provided `input`. `deepeval`'s answer relevancy metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Arguments\n",
    "To use the AnswerRelevancyMetric, you'll have to provide the following arguments when creating an LLMTestCase:\n",
    "\n",
    "- `input`\n",
    "- `actual_output`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucination\n",
    "The hallucination metric determines whether your LLM generates factually correct information by comparing the `actual_output` to the provided `context`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Arguments\n",
    "To use the HallucinationMetric, you'll have to provide the following arguments when creating an LLMTestCase:\n",
    "\n",
    "- `input`\n",
    "- `actual_output`\n",
    "- `retrieval_context`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Dataset <a class=\"anchor\" id=\"three-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Load evaluation dataset for generation evaluation\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "dataset = EvaluationDataset()\n",
    "dataset.add_test_cases_from_json_file(\n",
    "    # file_path is the absolute path to you .json file\n",
    "    file_path=\"example.json\",\n",
    "    input_key_name=\"query\",\n",
    "    actual_output_key_name=\"actual_output\",\n",
    "    expected_output_key_name=\"expected_output\",\n",
    "    context_key_name=\"context\",\n",
    "    retrieval_context_key_name=\"retrieval_context\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Playground <a class=\"anchor\" id=\"four-section\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set default prompts. **Do an evaluation run through this notebook before your first experiment.** This will set a baseline for you to compare your experiment results against.\n",
    "\n",
    "After you have done your first run through the notebook, please experiment with these prompts as you see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to experiment with:\n",
    "1. _chat_template\n",
    "2. CONDENSE_QUESTION_PROMPT\n",
    "3. _core_redbox_prompt\n",
    "4. CORE_REDBOX_PROMPT\n",
    "5. _with_sources_template\n",
    "6. WITH_SOURCES_PROMPT\n",
    "7. _stuff_document_template\n",
    "8. STUFF_DOCUMENT_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refining Question Prompts\n",
    "As this refining of the question is pre-retrieval, should we **remove it** from this generation evalution notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_chat_template = \"\"\"Given the following conversation and a follow up question,\n",
    "rephrase the follow up question to be a standalone question, in its original\n",
    "language. include the follow up instructions in the standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG prompts\n",
    "These are the prompts that will have most effect on RAG generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_core_redbox_prompt = \"\"\"You are RedBox Copilot. An AI focused on helping UK Civil Servants, Political Advisors and\\\n",
    "Ministers triage and summarise information from a wide variety of sources. You are impartial and\\\n",
    "non-partisan. You are not a replacement for human judgement, but you can help humans\\\n",
    "make more informed decisions. If you are asked a question you cannot answer based on your following instructions, you\\\n",
    "should say so. Be concise and professional in your responses. Respond in markdown format.\n",
    "\n",
    "=== RULES ===\n",
    "\n",
    "All responses to Tasks **MUST** be translated into the user's preferred language.\\\n",
    "This is so that the user can understand your responses.\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where CORE_REDBOX_PROMPT is used in the codebase\n",
    "CORE_REDBOX_PROMPT = PromptTemplate.from_template(_core_redbox_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_with_sources_template = \"\"\"Given the following extracted parts of a long document and \\\n",
    "a question, create a final answer with Sources at the end.  \\\n",
    "If you don't know the answer, just say that you don't know. Don't try to make \\\n",
    "up an answer.\n",
    "Be concise in your response and summarise where appropriate. \\\n",
    "At the end of your response add a \"Sources:\" section with the documents you used. \\\n",
    "DO NOT reference the source documents in your response. Only cite at the end. \\\n",
    "ONLY PUT CITED DOCUMENTS IN THE \"Sources:\" SECTION AND NO WHERE ELSE IN YOUR RESPONSE. \\\n",
    "IT IS CRUCIAL that citations only happens in the \"Sources:\" section. \\\n",
    "This format should be <DocX> where X is the document UUID being cited.  \\\n",
    "DO NOT INCLUDE ANY DOCUMENTS IN THE \"Sources:\" THAT YOU DID NOT USE IN YOUR RESPONSE. \\\n",
    "YOU MUST CITE USING THE <DocX> FORMAT. NO OTHER FORMAT WILL BE ACCEPTED.\n",
    "Example: \"Sources: <DocX> <DocY> <DocZ>\"\n",
    "\n",
    "Use **bold** to highlight the most question relevant parts in your response.\n",
    "If dealing dealing with lots of data return it in markdown table format.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "FINAL ANSWER:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WITH_SOURCES_PROMPT = PromptTemplate.from_template(_core_redbox_prompt + _with_sources_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stuff_document_template = \"<Doc{parent_doc_uuid}>{page_content}</Doc{parent_doc_uuid}>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUFF_DOCUMENT_PROMPT = PromptTemplate.from_template(_stuff_document_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find changes to the prompts above improve the generation evaluation scores, please consider making a PR to update the code in main.\n",
    "\n",
    "All these prompts are locations in [chat.py](../../redbox/llm/prompts/chat.py), except `_core_redbox_prompt` which is located in [core.py](../../redbox/llm/prompts/core.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate `actual_output` using RAG and evaluation dataset <a class=\"anchor\" id=\"five-section\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat(chat_request: ChatRequest, user_uuid: Annotated[UUID, Depends(get_user_uuid)]) -> ChatResponse:\n",
    "    \"\"\"Get a LLM response to a question history and file\n",
    "\n",
    "    Args:\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        StreamingResponse: a stream of the chain response\n",
    "    \"\"\"\n",
    "    question = chat_request.message_history[-1].text\n",
    "    previous_history = list(chat_request.message_history[:-1])\n",
    "    previous_history = ChatPromptTemplate.from_messages(\n",
    "        (msg.role, msg.text) for msg in previous_history\n",
    "    ).format_messages()\n",
    "\n",
    "    docs_with_sources_chain = load_qa_with_sources_chain(\n",
    "        llm,\n",
    "        chain_type=\"stuff\",\n",
    "        prompt=WITH_SOURCES_PROMPT,\n",
    "        document_prompt=STUFF_DOCUMENT_PROMPT,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    condense_question_chain = LLMChain(llm=llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "    standalone_question = condense_question_chain({\"question\": question, \"chat_history\": previous_history})[\"text\"]\n",
    "\n",
    "    docs = vector_store.as_retriever(\n",
    "        search_kwargs={\"filter\": {\"term\": {\"creator_user_uuid.keyword\": str(user_uuid)}}}\n",
    "    ).get_relevant_documents(standalone_question)\n",
    "\n",
    "    result = docs_with_sources_chain(\n",
    "        {\n",
    "            \"question\": standalone_question,\n",
    "            \"input_documents\": docs,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    source_documents = [\n",
    "        SourceDocument(\n",
    "            page_content=langchain_document.page_content,\n",
    "            file_uuid=langchain_document.metadata.get(\"parent_doc_uuid\"),\n",
    "            page_numbers=langchain_document.metadata.get(\"page_numbers\"),\n",
    "        )\n",
    "        for langchain_document in result.get(\"input_documents\", [])\n",
    "    ]\n",
    "    return ChatResponse(output_text=result[\"output_text\"], source_documents=source_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset for evaluation\n",
    "Process the goldens and convert them into test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A hypothetical LLM application example\n",
    "from chatbot import query\n",
    "from typing import List\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import Golden\n",
    "...\n",
    "\n",
    "def convert_goldens_to_test_cases(goldens: List[Golden]) -> List[LLMTestCase]:\n",
    "    test_cases = []\n",
    "    for golden in goldens:\n",
    "        test_case = LLMTestCase(\n",
    "            input=golden.input,\n",
    "            # Generate actual output using the 'input' and 'additional_metadata'\n",
    "            actual_output=query(golden.input, golden.additional_metadata),\n",
    "            expected_output=golden.expected_output,\n",
    "            context=golden.context,\n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "    return test_cases\n",
    "\n",
    "# Data preprocessing before setting the dataset test cases\n",
    "dataset.test_cases = convert_goldens_to_test_cases(dataset.goldens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andy/Library/Caches/pypoetry/virtualenvs/redbox-MiicHf1r-py3.11/lib/python3.11/site-packages/deepeval/__init__.py:42: UserWarning: You are using deepeval version 0.21.36, however version 0.21.42 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redbox-MiicHf1r-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
