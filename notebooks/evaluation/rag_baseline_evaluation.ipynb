{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Add autoreloatd\n",
            "%reload_ext autoreload\n",
            "%autoreload 2"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "metadata": {},
         "outputs": [],
         "source": [
            "from jose import jwt\n",
            "from uuid import UUID\n",
            "import requests\n",
            "import json\n",
            "import pandas as pd\n",
            "import pickle\n",
            "from dataclasses import asdict\n",
            "from pathlib import Path\n",
            "import jsonlines\n",
            "from elasticsearch import Elasticsearch\n",
            "\n",
            "from redbox.models import Settings\n",
            "\n",
            "from dotenv import find_dotenv, load_dotenv\n",
            "_ = load_dotenv(find_dotenv())\n",
            "\n",
            "pd.set_option(\"display.max_colwidth\", None)\n",
            "\n",
            "ENV = Settings()\n",
            "ENV.minio_host = \"localhost\"\n",
            "ENV.elastic.host = \"localhost\""
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "☝️⚠️ _expand cell to run imports_"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Set baseline evaluation for current/deployed Redbox RAG chat endpoint  <a class=\"anchor\" id=\"title\"></a>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "-----------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Evaluate Redbox RAG chat on one stable, numbered version of these data**"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "-----------------------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Table of Contents <a class=\"anchor\" id=\"toc\"></a>\n",
            "\n",
            "* 1. Introduction\n",
            "    * [Overview](#overview)\n",
            "    * [Metrics](#metrics)\n",
            "        - [Contextual Precision]()\n",
            "        - [Contextual Recall]()\n",
            "        - [Contextual Relevancy]()\n",
            "        - [Fathfulness]()\n",
            "        - [Answer Relevancy]()\n",
            "        - [Hallucination]()\n",
            "* 2. Setup\n",
            "    * [Set version of the evaluation dataset](#setversion)\n",
            "    * [Run Redbox locally](#run-redbox)\n",
            "    * [Load embeddings into the index](#load-embeddings)\n",
            "* 3. Set baseline\n",
            "    * [Get files that correspond to the version of evaluation dataset](#bl-files)\n",
            "    * [Load Evaluation Dataset into test cases](#bl-load-test-cases)\n",
            "    * [Generate `actual_output` using RAG and evaluation dataset](#bl-evaluate)\n",
            "        - [Retrieval Evaluation Metrics]()\n",
            "        - [Generation Evaluation Metrics]()\n",
            "    * [Analyse evaluation results](#bl-analysis)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "------------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 1. Introduction"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Overview <a class=\"anchor\" id=\"overview\"></a>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "This notebook allows you to use the Redbox RAG chat **endpoint** to set baseline evaluation metrics for the current/deployed RAG chat endpoint.\n",
            "\n",
            "Redbox RAG chat is made up of many components that work together to give the final RAG pipeline. Each component can be optimised to improve the overall performance of the RAG pipeline for Redbox tasks. In order to track if changes made are improving or degrading Redbox performance, we need to establish an evaluation framework. The overall RAG pipeline can be broken down into two main parts:\n",
            "\n",
            "1. Retrieval - searching and returning the most relevant documents to answer a user question\n",
            "2. Generation - the ouput of the LLM after considering the retrieved documents, any prompts provided and the user question\n",
            "\n",
            "This notebook tests both the retrieval and generation sides of the RAG pipeline using specific metrics for each, using the `DeepEval` framework.\n",
            "\n",
            "For consistency across the team, it is important to evaluate Redbox RAG chat on one stable, numbered version of these data."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Metrics <a class=\"anchor\" id=\"metrics\"></a>\n",
            "\n",
            "Retrieval metrics\n",
            "- Contextual Precision\n",
            "- Contextual Recall\n",
            "- Contextual Relevancy\n",
            "\n",
            "Generation metrics\n",
            "- Faithfulness\n",
            "- Answer Relevancy\n",
            "- Hallucination\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Contextual Precision\n",
            "\n",
            "The contextual precision metric measures your RAG pipeline's retriever by evaluating whether nodes in your `retrieval_context` that are relevant to the given `input` are ranked higher than irrelevant ones."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Contextual Recall\n",
            "\n",
            "The contextual recall metric measures the quality of your RAG pipeline's retriever by evaluating the extent of which the `retrieval_context` aligns with the `expected_output`."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Contextual Relevancy\n",
            "\n",
            "The contextual relevancy metric measures the quality of your RAG pipeline's retriever by evaluating the overall relevance of the information presented in your `retrieval_context` for a given `input`."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Faithfulness\n",
            "\n",
            "The faithfulness metric measures the quality of your RAG pipeline's generator by evaluating whether the `actual_output` factually aligns with the contents of your `retrieval_context`. `deepeval`'s faithfulness metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Required Arguments\n",
            "To use the `FaithfulnessMetric`, you need to provide the following arguments when creating an LLMTestCase:\n",
            "\n",
            "- `input`\n",
            "- `actual_output`\n",
            "- `retrieval_context`"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Answer Relevancy\n",
            "The answer relevancy metric measures the quality of your RAG pipeline's generator by evaluating how relevant the actual_output of your LLM application is compared to the provided `input`. `deepeval`'s answer relevancy metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Required Arguments\n",
            "To use the AnswerRelevancyMetric, you'll have to provide the following arguments when creating an LLMTestCase:\n",
            "\n",
            "- `input`\n",
            "- `actual_output`"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Hallucination\n",
            "The hallucination metric determines whether your LLM generates factually correct information by comparing the `actual_output` to the provided `context`."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Required Arguments\n",
            "To use the HallucinationMetric, you'll have to provide the following arguments when creating an LLMTestCase:\n",
            "\n",
            "- `input`\n",
            "- `actual_output`\n",
            "- `retrieval_context`"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "-------------------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 2. Setup"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Set global variables <a class=\"anchor\" id=\"setversion\"></a>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Evaluate Redbox RAG chat on one stable, numbered version of these data**"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Set the version of the evaluation dataset you are using to evalute Redbox in the cell below**   <a class=\"anchor\" id=\"setversion\"></a>"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "DATA_VERSION = \"0.1.0\""
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Embedding and retrieval is locked to a particular embedding model, which should be tied to a single index in the vector stoer. Here we default to the `EMBEDDING_MODEL` environment variable, which will match production if set via `.env.example`."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "MODEL = ENV.embedding_model\n",
            "INDEX = f\"{DATA_VERSION}-{MODEL}\".lower()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Run the cell below to set up the required folder structure (it will not overwrite folders and files if they already exist)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "ROOT = Path.cwd().parents[1]\n",
            "EVALUATION_DIR = ROOT / \"notebooks/evaluation\"\n",
            "\n",
            "V_ROOT = EVALUATION_DIR / f\"data/{DATA_VERSION}\"\n",
            "V_RAW = V_ROOT / \"raw\"\n",
            "V_SYNTHETIC = V_ROOT / \"synthetic\"\n",
            "V_CHUNKS = V_ROOT / \"chunks\"\n",
            "V_RESULTS = V_ROOT / \"results\"\n",
            "V_EMBEDDINGS = V_ROOT / \"embeddings\"\n",
            "\n",
            "V_ROOT.mkdir(parents=True, exist_ok=True)\n",
            "V_RAW.mkdir(parents=True, exist_ok=True)\n",
            "V_SYNTHETIC.mkdir(parents=True, exist_ok=True)\n",
            "V_CHUNKS.mkdir(parents=True, exist_ok=True)\n",
            "V_RESULTS.mkdir(parents=True, exist_ok=True)\n",
            "V_EMBEDDINGS.mkdir(parents=True, exist_ok=True)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "To save on API costs, we only need to generate a particular version of the evaluation dataset once. If you are using a previously generaterated evalutation dataset, **please download it from shared team location (Google Drive).**"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "It's helpful for all calls to share a dummy user. Set that here."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "USER_UUID = UUID(\"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Set up the clients to connect to the backend."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "INFO:root:Connecting to self managed Elasticsearch\n",
                  "INFO:root:Elasticsearch host = localhost\n"
               ]
            }
         ],
         "source": [
            "S3_CLIENT = ENV.s3_client()\n",
            "ES_CLIENT = ENV.elasticsearch_client()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Start Redbox locally <a id=\"run-redbox\"></a>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Start docker runtime, likely with Docker Desktop. However, if you are using colima run the following terminal command\n",
            "\n",
            "```bash\n",
            "colima start --memory 8\n",
            "``` "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### First-time setup"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "First time users need to do the following"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "```bash\n",
            "poetry install\n",
            "```"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Ensure your `.env` file has an OpenAI API key in, and has the following settings.\n",
            "\n",
            "Note `EMBEDDING_MODEL` must also be set, and should match both production, and the embeddings shared with the dataset. If not, go back to dataset creation, and embed the documents using a model that matches production."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "```bash\n",
            "# === Object Storage ===\n",
            "\n",
            "MINIO_HOST=minio\n",
            "MINIO_PORT=9000\n",
            "MINIO_ACCESS_KEY=minioadmin\n",
            "MINIO_SECRET_KEY=minioadmin\n",
            "AWS_ACCESS_KEY=minioadmin\n",
            "AWS_SECRET_KEY=minioadmin\n",
            "\n",
            "AWS_REGION=eu-west-2\n",
            "\n",
            "# minio or s3\n",
            "OBJECT_STORE=minio\n",
            "BUCKET_NAME=redbox-storage-dev\n",
            "```"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Run Redbox locally"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Every time you start Redbox for evaluation (no Django frontend required), please run the following command**"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "```bash\n",
            "make eval_backend\n",
            "````"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "The above command will bring up everything you need for the backend (`core-api`, `worker`, `mino`, `elasticsearch` and `redis`), then create the MinIO bucket needed to store raw files"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Load embeddings into the index <a id=\"load-embeddings\"></a>\n",
            "\n",
            "When baselining, we're working with chunks in the index used by the live system, `redbox-data-chunk`.\n",
            "\n",
            "Baselining is only meaningful if the only things in the index are the chunks from the dataset, so first we need to delete the existing data.\n",
            "\n",
            "⚠️ THIS CODE IS DESTRUCTIVE."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_94768/731841014.py:1: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
                  "  ES_CLIENT.indices.delete(index=\"redbox-data-file\", ignore=[400, 404])\n",
                  "INFO:elastic_transport.transport:DELETE http://localhost:9200/redbox-data-file [status:200 duration:0.172s]\n",
                  "/var/folders/14/6nvsrw1n2ls1xncz_bvy2x8m0000gq/T/ipykernel_94768/731841014.py:2: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
                  "  ES_CLIENT.indices.delete(index=\"redbox-data-chunk\", ignore=[400, 404])\n",
                  "INFO:elastic_transport.transport:DELETE http://localhost:9200/redbox-data-chunk [status:200 duration:0.138s]\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "ObjectApiResponse({'acknowledged': True})"
                  ]
               },
               "execution_count": 11,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "ES_CLIENT.indices.delete(index=\"redbox-data-file\", ignore=[400, 404])\n",
            "ES_CLIENT.indices.delete(index=\"redbox-data-chunk\", ignore=[400, 404])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now we're clear to load the pre-embedded chunks in."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [],
         "source": [
            "def load_chunks_from_jsonl_to_index(file_path: Path, es_client: Elasticsearch, index: str) -> None:\n",
            "    with jsonlines.open(file_path, mode=\"r\") as reader:\n",
            "        for chunk_raw in reader:\n",
            "            chunk = json.loads(chunk_raw)\n",
            "            es_client.index(\n",
            "                index=index,\n",
            "                id=chunk[\"uuid\"],\n",
            "                body=chunk,\n",
            "            )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "load_chunks_from_jsonl_to_index(\n",
            "    file_path=V_EMBEDDINGS / f\"{EMBEDDING_MODEL}.jsonl\",\n",
            "    es_client=ES_CLIENT,\n",
            "    index=INDEX\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "----------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# BASELINE <a id=\"baseline\"></a>\n",
            "\n",
            "Run through this notebook to establish baseline evaluation metrics for the current/deployed Redbox RAG chat endpoint"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Set evalution baseline using current Redbox Core-API"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Use the printed out bearer token below to Authorize if you ever want to use the Swagger UI docs**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "bearer_token = jwt.encode({\"user_uuid\": str(UUID(\"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\"))}, key=\"your-secret-key\", algorithm=\"HS512\")\n",
            "print(bearer_token)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### First need to upload files that we are going to 'RAG with'\n",
            "\n",
            "If files have already been uploaded you can skip this step"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Get files that correspond to the version of evaluation dataset  <a class=\"anchor\" id=\"files\"></a>\n",
            "\n",
            "Copy all the files that match your {DATA_VERSION} into `notebooks/evaluation/data/{DATA_VERSION}/raw/`. Find these files on the shared Google Drive and the corresponding version number/location\n",
            "\n",
            "**It is really important to use the same files that were used to genearte this particular version of the evaluation dataset. A mismatch between the two will result in inaccurate evaluatoin metrics**"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Only if you haven't uploaded files already** uncomment and run cell below"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# url = 'http://127.0.0.1:5002/file/upload'\n",
            "\n",
            "# headers={\n",
            "#     'accept': 'application/json',\n",
            "#     \"Authorization\": f\"Bearer {bearer_token}\"\n",
            "# }\n",
            "# for file in V_RAW.glob(\"*.*\"):\n",
            "#     files = {'file': open(file, 'rb')}\n",
            "#     upload_file_response = requests.post(url, headers=headers, files=files)\n",
            "\n",
            "#     #TODO: Add some login in the loop to deal with status codes != 200\n",
            "#     # if upload_file_response.status_code != 200:\n",
            "#     #     print(\"Failed to upload data:\", upload_file_response.status_code)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "List files uploaded to server & view JSON response"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "url = 'http://127.0.0.1:5002/file/'\n",
            "\n",
            "headers={\n",
            "    'accept': 'application/json',\n",
            "    \"Authorization\": f\"Bearer {bearer_token}\"\n",
            "}\n",
            "\n",
            "file_list_response = requests.get(url, headers=headers)\n",
            "\n",
            "if file_list_response.status_code == 200:\n",
            "    # Parse JSON from the response\n",
            "    data = file_list_response.json()\n",
            "    \n",
            "    # Pretty-print the JSON data\n",
            "    pretty_json = json.dumps(data, indent=4)\n",
            "    print(pretty_json)\n",
            "else:\n",
            "    print(\"Failed to retrieve data:\", file_list_response.status_code)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Get file status"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from redbox.models import FileStatus\n",
            "\n",
            "def pretty_upload_status(file_uuid: UUID, bearer_token: str) -> str:\n",
            "    headers={\n",
            "        'accept': 'application/json',\n",
            "        \"Authorization\": f\"Bearer {bearer_token}\"\n",
            "    }\n",
            "    status = FileStatus(**requests.get(f\"http://127.0.0.1:5002/file/{file_uuid}/status\", headers=headers).json())\n",
            "\n",
            "    status_title = status.processing_status.title()\n",
            "    n_chunks = 0\n",
            "    if status.chunk_statuses is not None:\n",
            "        n_chunks = len(status.chunk_statuses)\n",
            "\n",
            "    if status.processing_status == \"embedding\" and n_chunks > 0 and status.chunk_statuses is not None:\n",
            "        n_chunks_embedded = len([chunk for chunk in status.chunk_statuses if chunk.embedded])\n",
            "        return f\"{status_title} ({n_chunks_embedded / n_chunks:.0%})\"\n",
            "    else:\n",
            "        return status_title\n",
            "\n",
            "statuses = [pretty_upload_status(file[\"uuid\"], bearer_token) for file in file_list_response.json()]\n",
            "statuses"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "-------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**Please ensure all emeddings have been completed before proceeding!**\n",
            "\n",
            "Keep calm and go for a tea break!"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "--------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Generate `actual_output` & `retrieval_context` (using RAG chat endpoint)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df = pd.read_csv(f'{V_SYNTHETIC}/ragas_synthetic_data.csv')\n",
            "inputs = df['input'].tolist()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Using the RAG endpoint"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_endpoint = df.copy()\n",
            "\n",
            "retrieval_context = []\n",
            "actual_output = []\n",
            "\n",
            "headers = {\n",
            "    'accept': 'application/json',\n",
            "    'Authorization': 'Bearer ' + bearer_token,\n",
            "    'Content-Type': 'application/json',\n",
            "}\n",
            "\n",
            "url = 'http://127.0.0.1:5002/chat/rag'\n",
            "\n",
            "for question in inputs:\n",
            "    data = {\n",
            "        \"message_history\": [\n",
            "            {\n",
            "                \"role\": \"user\",\n",
            "                \"text\": question\n",
            "            }\n",
            "        ]\n",
            "    }\n",
            "    \n",
            "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
            "    data = response.json()\n",
            "\n",
            "    retrieval_context.append(data['source_documents'])\n",
            "    actual_output.append(data['output_text'])\n",
            "\n",
            "df_endpoint['actual_output'] = actual_output\n",
            "df_endpoint['retrieval_context'] = retrieval_context"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "You can keep an eye on progress by inspecting the core API's logs using `docker compose logs -f core-api`."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Confirm actual_output & retrieved_context added to the dataframe"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_endpoint.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Remove rows containing NaN to prevent Pydantic validation errors"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_clean = df_endpoint.dropna()\n",
            "df_clean.to_csv(f'{V_SYNTHETIC}/current_baseline_complete_endpoint_ragas_synthetic_data.csv', index=False)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "----------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Load Evaluation Dataset into test cases <a class=\"anchor\" id=\"load-test-cases\"></a>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Put the CSV file that you want to use for evaluation into `/notebooks/evaluation/data/synthetic_data/` directory"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Import test cases from CSV"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from deepeval.dataset import EvaluationDataset\n",
            "\n",
            "dataset_baseline = EvaluationDataset()\n",
            "dataset_baseline.add_test_cases_from_csv_file(\n",
            "    file_path=f'{V_SYNTHETIC}/current_baseline_complete_endpoint_ragas_synthetic_data.csv', # endpoint\n",
            "    input_col_name=\"input\",\n",
            "    actual_output_col_name=\"actual_output\",\n",
            "    expected_output_col_name=\"expected_output\",\n",
            "    context_col_name=\"context\",\n",
            "    context_col_delimiter= \";\",\n",
            "    retrieval_context_col_name=\"retrieval_context\",\n",
            "    retrieval_context_col_delimiter= \";\"\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "-------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Evaluate RAG pipeline <a id=\"evaluate\"></a>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "DeepEval imports"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from deepeval import evaluate\n",
            "from deepeval.metrics import (\n",
            "    ContextualPrecisionMetric,\n",
            "    ContextualRecallMetric,\n",
            "    ContextualRelevancyMetric,\n",
            "    AnswerRelevancyMetric,\n",
            "    FaithfulnessMetric,\n",
            "    HallucinationMetric,\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Instantiate retrieval and generation evaluation metrics"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Instantiate retrieval metrics\n",
            "contextual_precision = ContextualPrecisionMetric(\n",
            "    threshold=0.5, # default is 0.5\n",
            "    model=\"gpt-4o\",\n",
            "    include_reason=True\n",
            ")\n",
            "\n",
            "contextual_recall = ContextualRecallMetric(\n",
            "    threshold=0.5, # default is 0.5\n",
            "    model=\"gpt-4o\",\n",
            "    include_reason=True\n",
            ")\n",
            "\n",
            "contextual_relevancy = ContextualRelevancyMetric(\n",
            "    threshold=0.5, # default is 0.5\n",
            "    model=\"gpt-4o\",\n",
            "    include_reason=True\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Instantiate generation metrics\n",
            "answer_relevancy = AnswerRelevancyMetric(\n",
            "    threshold=0.5, # default is 0.5\n",
            "    model=\"gpt-4o\",\n",
            "    include_reason=True\n",
            ")\n",
            "\n",
            "faithfulness = FaithfulnessMetric(\n",
            "    threshold=0.5, # default is 0.5\n",
            "    model=\"gpt-4o\",\n",
            "    include_reason=True\n",
            ")\n",
            "\n",
            "hallucination = HallucinationMetric(\n",
            "    threshold=0.5, # default is 0.5\n",
            "    model=\"gpt-4o\",\n",
            "    include_reason=True\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### View test cases"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset_baseline.test_cases"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Baseline Evaluation"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "baseline_eval_results = evaluate(\n",
            "    test_cases=dataset_baseline,\n",
            "    metrics=[\n",
            "        contextual_precision,\n",
            "        contextual_recall,\n",
            "        contextual_relevancy,\n",
            "        answer_relevancy,\n",
            "        faithfulness,\n",
            "        hallucination\n",
            "    ]\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Save baseline evaluation results"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "with open(f\"{V_RESULTS}/current_baseline_eval_results_v{DATA_VERSION}\", \"wb\") as f:\n",
            "    pickle.dump(baseline_eval_results, f)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "with open(f\"{V_RESULTS}/current_baseline_eval_results_v{DATA_VERSION}\", \"rb\") as f:\n",
            "    baseline_eval_results = pickle.load(f)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "-------"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Analyse baseline evaluation results <a id=\"analysis\"></a>"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "metric_type = {\n",
            "    \"metric_name\": [\"Contextual Precision\", \"Contextual Recall\", \"Contextual Relevancy\", \"Answer Relevancy\", \"Faithfulness\", \"Hallucination\"],\n",
            "    \"metric_type\": [\"retrieval\", \"retrieval\", \"retrieval\", \"generation\", \"generation\", \"generation\"]\n",
            "}\n",
            "\n",
            "baseline = (\n",
            "    pd.DataFrame.from_records(\n",
            "        asdict(result) for result in baseline_eval_results\n",
            "    )\n",
            "    .explode(\"metrics\")\n",
            "    .reset_index(drop=True)\n",
            "    .assign(\n",
            "        metric_name = lambda df: df.metrics.apply(getattr, args=[\"__name__\"]),\n",
            "        score = lambda df: df.metrics.apply(getattr, args=[\"score\"]),\n",
            "        reason = lambda df: df.metrics.apply(getattr, args=[\"reason\"])\n",
            "    )\n",
            "    .merge(\n",
            "        pd.DataFrame(metric_type),\n",
            "        on=\"metric_name\"\n",
            "    )\n",
            "    .drop(columns=[\"success\"])\n",
            ")\n",
            "\n",
            "baseline.to_csv(f\"{V_RESULTS}/baseline.csv\", index=False)\n",
            "baseline.head()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Baseline aggregated view for current Redbox RAG chat core API endpoint"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "(\n",
            "    baseline\n",
            "    .groupby([\"metric_name\", \"metric_type\"])\n",
            "    .mean(\"score\")\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "[Back to top](#title)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "-----------"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "redbox-MiicHf1r-py3.11",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.8"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
